# ğŸš€ Temporal Fusion Transformer for Predictive Maintenance  
**Remaining Useful Life (RUL) prediction on NASA C-MAPSS turbofan engine dataset using Temporal Fusion Transformer (TFT), with interpretable ML and feature engineering.**  
ğŸ“Œ Status: Work in progress (v0.1) â€” code, notebooks, and visualizations being released in stages.

---

## ğŸ” 1. Research Summary
This project investigates how long-range temporal dependencies across multi-sensor streams influence Remaining Useful Life (RUL) prediction in turbofan engines.  
Traditional LSTM/GRU models struggle to capture cross-sensor interactions over long horizons; we explore **Temporal Fusion Transformer (TFT)** as a more expressive architecture for industrial prognostics.

**Research questions**  
1. Can attention-based models outperform recurrent baselines for RUL forecasting?  
2. Which sensors contribute most to late-stage failure signals?  
3. How interpretable are temporal attention patterns for engineering decision-making?

---

## ğŸ§  2. Methods & Model
âœ… Model: Temporal Fusion Transformer (PyTorch)  
âœ… Baselines: LSTM, GRU  
âœ… Feature Engineering: rolling-window stats, spectral transforms, cross-sensor correlations  
âœ… Interpretation: SHAP + attention visualisation

ğŸ“Œ Notebook version also provided in `/notebooks/tft_rul.ipynb`

---

## ğŸ“‚ 3. Dataset
| Source | NASA C-MAPSS |  
|--------|--------------|  
| Type | Multi-sensor turbofan degradation simulation |  
| Samples | 4 subsets Ã— 26 sensors Ã— 20k+ cycles |  
| Target | RUL (Remaining Useful Life regression) |

ğŸ”— Dataset download instructions included in `data/README.md`  
(also mirrored to Kaggle for fast access)

---

## ğŸ“Š 4. Preliminary Results *(placeholder â€” updated after full release)*  

| Model | RMSE â†“ | Notes |
|--------|--------|-------|
| LSTM | 32.8 | Baseline |
| GRU | 31.5 | Baseline |
| **TFT** | **25.4** | âœ… ~20% improvement |

ğŸ“Œ Result plot (placeholder â€” will be replaced when code execution completes):

![results](assets/results_placeholder.png)

---

## ğŸ§  5. Model Interpretability *(to be verified after training)*
SHAP and TFT variable-selection visualizations will be added after the first successful run.

![shap](assets/shap_placeholder.png)

*Note:* Sensor importance patterns and any cross-sensor interactions will be reported only after empirical verification.


---

## ğŸ§ª 6. Reproducible Workflow
```bash
git clone https://github.com/ivy88853/tft-predictive-maintenance.git
cd tft-predictive-maintenance
pip install -r requirements.txt
python src/train.py --model tft

tft-predictive-maintenance
 â”œâ”€â”€ assets/               # figures, architecture diagrams
 â”œâ”€â”€ data/                 # dataset download + preprocessing
 â”œâ”€â”€ notebooks/            # Jupyter version
 â”œâ”€â”€ src/                  # full PyTorch pipeline
 â”œâ”€â”€ requirements.txt
 â””â”€â”€ README.md
Zhan, I. (2024). Temporal Fusion Transformer for Predictive Maintenance on NASA Turbofan Data. GitHub Repository.
## ğŸ“¬ Contact
Maintainer: Hong Zhan (Ivy)  
ğŸ“ London, United Kingdom  
ğŸ“§ Email: ivy88853@gmail.com  
ğŸ”— ResearchGate: coming soon  

